{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1B5kyIvySAGOYmiZ69RcJCNP-f8gYZ4IW","authorship_tag":"ABX9TyNuMnVKmzvozyclRTbNfZYV"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"rGqR1e12U_2L"},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow.keras.applications import resnet50, xception, inception_v3  \n","from keras import Input, Model, layers"]},{"cell_type":"code","source":["import pathlib\n","\n","BATCH_SIZE = 32\n","IMG_SIZE = (224, 224)\n","\n","data_dir = pathlib.Path('drive/My Drive/TIES4911/wonders_world/Wonders of World/Wonders of World')\n","(train_ds, val_ds) = tf.keras.utils.image_dataset_from_directory(\n","    data_dir,\n","    shuffle=True,\n","    validation_split=0.2,\n","    subset='both',\n","    image_size=IMG_SIZE,\n","    batch_size=BATCH_SIZE,\n","    seed=123,\n","    label_mode='categorical'\n",")\n","\n","class_names = train_ds.class_names\n","num_classes = len(class_names)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7gASzWk0VHpf","executionInfo":{"status":"ok","timestamp":1677609953109,"user_tz":-120,"elapsed":6643,"user":{"displayName":"Tr창m Mai","userId":"14047835568893253890"}},"outputId":"de9e8c48-77b5-4bb2-a285-9c62352921fc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 3846 files belonging to 12 classes.\n","Using 3077 files for training.\n","Using 769 files for validation.\n"]}]},{"cell_type":"code","source":["AUTOTUNE = tf.data.AUTOTUNE\n","\n","train_ds = train_ds.prefetch(buffer_size=AUTOTUNE)\n","val_ds = val_ds.prefetch(buffer_size=AUTOTUNE)"],"metadata":{"id":"tsB3_raSVJCH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_augmentation = tf.keras.Sequential([\n","  tf.keras.layers.RandomFlip('horizontal'),\n","  tf.keras.layers.RandomRotation(0.2),\n","])"],"metadata":{"id":"Du1hv4N0VO6J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["inputs = Input(shape=(224, 224, 3))\n","\n","base_model = xception.Xception(input_tensor=inputs, include_top=False)\n","base_model.trainable = False\n","\n","preprocess_input = xception.preprocess_input\n","global_average_layer = layers.GlobalAveragePooling2D()\n","prediction_layer = layers.Dense(12)\n","\n","x = data_augmentation(inputs)\n","x = preprocess_input(x)\n","x = base_model(x, training=False)\n","x = global_average_layer(x)\n","x = layers.Dropout(0.5)(x)\n","outputs = prediction_layer(x)\n","\n","model = Model(inputs, outputs)\n","model.compile(optimizer='adam',\n","              loss='categorical_crossentropy',\n","              metrics=['accuracy'])\n","\n","checkpoint = tf.train.Checkpoint(model=model)\n","model.summary()\n","\n","loss0, accuracy0 = model.evaluate(val_ds)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9IgLBUntVQ0h","executionInfo":{"status":"ok","timestamp":1677610110073,"user_tz":-120,"elapsed":133967,"user":{"displayName":"Tr창m Mai","userId":"14047835568893253890"}},"outputId":"1e2acfd1-27de-48ba-ecca-0a6c9f0c12a8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/xception/xception_weights_tf_dim_ordering_tf_kernels_notop.h5\n","83683744/83683744 [==============================] - 1s 0us/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n"]},{"output_type":"stream","name":"stdout","text":["Model: \"model\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n","                                                                 \n"," sequential (Sequential)     (None, 224, 224, 3)       0         \n","                                                                 \n"," tf.math.truediv (TFOpLambda  (None, 224, 224, 3)      0         \n"," )                                                               \n","                                                                 \n"," tf.math.subtract (TFOpLambd  (None, 224, 224, 3)      0         \n"," a)                                                              \n","                                                                 \n"," xception (Functional)       (None, 7, 7, 2048)        20861480  \n","                                                                 \n"," global_average_pooling2d (G  (None, 2048)             0         \n"," lobalAveragePooling2D)                                          \n","                                                                 \n"," dropout (Dropout)           (None, 2048)              0         \n","                                                                 \n"," dense (Dense)               (None, 12)                24588     \n","                                                                 \n","=================================================================\n","Total params: 20,886,068\n","Trainable params: 24,588\n","Non-trainable params: 20,861,480\n","_________________________________________________________________\n","25/25 [==============================] - 130s 5s/step - loss: 7.9358 - accuracy: 0.1157\n"]}]},{"cell_type":"code","source":["print(len(model.trainable_variables))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3FPfmJSaWZpT","executionInfo":{"status":"ok","timestamp":1677610119191,"user_tz":-120,"elapsed":805,"user":{"displayName":"Tr창m Mai","userId":"14047835568893253890"}},"outputId":"473bd62a-6ed4-4c83-89b7-309400a1f9ef"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2\n"]}]},{"cell_type":"code","source":["checkpoint_path = 'drive/My Drive/TIES4911/task4/task4-2/xception-{epoch:04d}'\n","\n","model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n","    filepath=checkpoint_path,\n","    save_weights_only=True,\n","    save_freq=2*97,\n","    verbose=1)\n","\n","history = model.fit(train_ds,\n","                    epochs=10,\n","                    validation_data=val_ds,\n","                    callbacks=[model_checkpoint_callback])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Uy1hXgeJVSpn","executionInfo":{"status":"ok","timestamp":1677616376703,"user_tz":-120,"elapsed":3136474,"user":{"displayName":"Tr창m Mai","userId":"14047835568893253890"}},"outputId":"54b4eebf-9843-47b7-fb08-8f17e7436ec5"},"execution_count":null,"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch 1/10\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n"]},{"output_type":"stream","name":"stdout","text":["97/97 [==============================] - 642s 6s/step - loss: 9.3550 - accuracy: 0.1261 - val_loss: 10.0418 - val_accuracy: 0.1469\n","Epoch 2/10\n","96/97 [============================>.] - ETA: 4s - loss: 10.6606 - accuracy: 0.1445\n","Epoch 2: saving model to drive/My Drive/TIES4911/task4/task4-2/xception-0002\n","97/97 [==============================] - 575s 6s/step - loss: 10.6694 - accuracy: 0.1449 - val_loss: 12.4920 - val_accuracy: 0.1847\n","Epoch 3/10\n","97/97 [==============================] - 565s 6s/step - loss: 11.2464 - accuracy: 0.1518 - val_loss: 12.8064 - val_accuracy: 0.1847\n","Epoch 4/10\n","96/97 [============================>.] - ETA: 4s - loss: 11.1583 - accuracy: 0.1569\n","Epoch 4: saving model to drive/My Drive/TIES4911/task4/task4-2/xception-0004\n","97/97 [==============================] - 573s 6s/step - loss: 11.1612 - accuracy: 0.1573 - val_loss: 12.4711 - val_accuracy: 0.2016\n","Epoch 5/10\n","97/97 [==============================] - 591s 6s/step - loss: 10.9926 - accuracy: 0.1547 - val_loss: 13.0160 - val_accuracy: 0.2081\n","Epoch 6/10\n","96/97 [============================>.] - ETA: 4s - loss: 11.1246 - accuracy: 0.1582\n","Epoch 6: saving model to drive/My Drive/TIES4911/task4/task4-2/xception-0006\n","97/97 [==============================] - 594s 6s/step - loss: 11.1275 - accuracy: 0.1579 - val_loss: 13.2466 - val_accuracy: 0.2081\n","Epoch 7/10\n","97/97 [==============================] - 591s 6s/step - loss: 11.2688 - accuracy: 0.1631 - val_loss: 13.3304 - val_accuracy: 0.2016\n","Epoch 8/10\n","96/97 [============================>.] - ETA: 4s - loss: 11.1505 - accuracy: 0.1680\n","Epoch 8: saving model to drive/My Drive/TIES4911/task4/task4-2/xception-0008\n","97/97 [==============================] - 592s 6s/step - loss: 11.1533 - accuracy: 0.1680 - val_loss: 13.4143 - val_accuracy: 0.1964\n","Epoch 9/10\n","97/97 [==============================] - 598s 6s/step - loss: 11.4256 - accuracy: 0.1573 - val_loss: 13.2885 - val_accuracy: 0.1834\n","Epoch 10/10\n","96/97 [============================>.] - ETA: 4s - loss: 11.6024 - accuracy: 0.1663\n","Epoch 10: saving model to drive/My Drive/TIES4911/task4/task4-2/xception-0010\n","97/97 [==============================] - 597s 6s/step - loss: 11.6045 - accuracy: 0.1661 - val_loss: 13.0789 - val_accuracy: 0.1834\n"]}]},{"cell_type":"code","source":["loss1, accuracy1 = model.evaluate(val_ds)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kqIiBQWRtmWf","executionInfo":{"status":"ok","timestamp":1677616487190,"user_tz":-120,"elapsed":110493,"user":{"displayName":"Tr창m Mai","userId":"14047835568893253890"}},"outputId":"b2a703ae-a085-456a-fcd5-501b0fea4f00"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["25/25 [==============================] - 111s 4s/step - loss: 13.0789 - accuracy: 0.1834\n"]}]}]}
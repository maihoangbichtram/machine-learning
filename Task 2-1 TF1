{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPo5lVcoZ7gMaUUOPhz47Nk"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"x7D0MN6AWWKv"},"outputs":[],"source":["# Single layer\n","import tensorflow.compat.v1 as tf\n","tf.compat.v1.disable_eager_execution()\n","from tensorflow.keras.datasets import fashion_mnist\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n","x_train = tf.reshape(x_train,[60000,784])\n","x_test = tf.reshape(x_test,[10000,784])\n","\n","x_train /= 255\n","x_test /= 255\n","unique_category_count = 10\n","y_train = tf.one_hot(y_train, unique_category_count)\n","y_test = tf.one_hot(y_test, unique_category_count)\n","\n","learning_rate = 0.001\n","epochs = 1\n","batch_size = 128\n","display_step = 1\n","\n","n_hidden_1 = 256\n","n_input = 784\n","n_classes = 10\n","\n","x = tf.placeholder('float', [None, n_input])\n","y = tf.placeholder('float', [None, n_classes])\n","\n","def singlelayer_perceptron(x):\n","  weights = {\n","      'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n","      'out': tf.Variable(tf.random_normal([n_hidden_1, n_classes]))\n","  }\n","  biases = {\n","      'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n","      'out': tf.Variable(tf.random_normal([n_classes]))\n","  }\n","  print( 'x:', x.get_shape(), 'W1:', weights['h1'].get_shape(), 'b1:', biases['b1'].get_shape())\n","  layer = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n","  layer = tf.nn.relu(layer)\n","  print( 'layer_1:', layer.get_shape(), 'W2:', weights['out'].get_shape(), 'b2:', biases['out'].get_shape())\n","  out_layer = tf.add(tf.matmul(layer, weights['out']), biases['out'])\n","  print('out_layer:',out_layer.get_shape())\n","\n","  return out_layer\n","\n","\n","pred = singlelayer_perceptron(x)\n","cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y, logits=pred))\n","optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n","init = tf.global_variables_initializer()\n","\n","with tf.Session() as sess:\n","  sess.run(init)\n","  # Training cycle\n","  for epoch in range(epochs):\n","    print('Epoch: ', epoch)\n","    avg_cost = 0.\n","    total_batch = int(x_train.shape[0]/batch_size)\n","    print('total_batch: ', total_batch)\n","    # Loop over all batches\n","    for i in range(total_batch):\n","      print('i: ', i)\n","      batch_xs = x_train[i * batch_size: (i+1) * batch_size]\n","      batch_ys = y_train[i * batch_size: (i+1) * batch_size]\n","      # Run optimization op (backprop) and cost op (to get loss value)\n","      _, c = sess.run([optimizer, cost], feed_dict={x: sess.run(batch_xs), y: sess.run(batch_ys)})\n","      # Compute average loss\n","      avg_cost += c / total_batch\n","    # Display logs per epoch step\n","    if epoch % display_step == 0:\n","      print (\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(avg_cost))\n","  print(\"Optimization Finished!\")\n","  # Test model\n","  correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n","  # Calculate accuracy\n","  accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n","  # To keep sizes compatible with model\n","  print (\"Accuracy:\", accuracy.eval({x: sess.run(x_test), y: sess.run(y_test)}))\n","  print(\"Test sample:\")\n","  #Get 28x28 image\n","  sample_1 = tf.reshape(x_test[47], [28,28])\n","  # Get corresponding integer label from one-hot encoded data\n","  sample_label_1 = np.where(sess.run(y_test[47]) == 1)[0][0]\n","  # Plot sample\n","  plt.imshow(sess.run(sample_1), cmap='Greys')\n","  plt.title('label = {}'.format(sample_label_1))\n","  plt.show()\n","  x_test = tf.reshape(x_test[47], [1,784])\n","  predicted_value = tf.argmax(pred, 1)\n","  print(\"is classified as: {}\".format(predicted_value.eval(session=sess, feed_dict={x: sess.run(x_test)})))"]},{"cell_type":"code","source":["# Multi layer\n","\n","n_hidden_2 = 256\n","\n","def multilayer_perceptron(x):\n","  weights = {\n","      'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n","      'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n","      'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes]))\n","  }\n","  biases = {\n","      'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n","      'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n","      'out': tf.Variable(tf.random_normal([n_classes]))\n","  }\n","  print( 'x:', x.get_shape(), 'W1:', weights['h1'].get_shape(), 'b1:', biases['b1'].get_shape())\n","  layer = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n","  layer = tf.nn.relu(layer)\n","  print( 'layer_1:', layer.get_shape(), 'W2:', weights['h2'].get_shape(), 'b2:', biases['b2'].get_shape())\n","  layer_2 = tf.add(tf.matmul(layer, weights['h2']), biases['b2'])\n","  layer_2 = tf.nn.relu(layer_2)\n","  print( 'layer_2:', layer.get_shape(), 'W3:', weights['out'].get_shape(), 'b3:', biases['out'].get_shape())\n","  out_layer = tf.add(tf.matmul(layer_2, weights['out']), biases['out'])\n","  print('out_layer:',out_layer.get_shape())\n","\n","  return out_layer\n","\n","\n","pred = multilayer_perceptron(x)\n","cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y, logits=pred))\n","optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n","init = tf.global_variables_initializer()\n","\n","with tf.Session() as sess:\n","  sess.run(init)\n","  # Training cycle\n","  for epoch in range(epochs):\n","    print('Epoch: ', epoch)\n","    avg_cost = 0.\n","    total_batch = int(x_train.shape[0]/batch_size)\n","    print('total_batch: ', total_batch)\n","    # Loop over all batches\n","    for i in range(total_batch):\n","      print('i: ', i)\n","      batch_xs = x_train[i * batch_size: (i+1) * batch_size]\n","      batch_ys = y_train[i * batch_size: (i+1) * batch_size]\n","      # Run optimization op (backprop) and cost op (to get loss value)\n","      _, c = sess.run([optimizer, cost], feed_dict={x: sess.run(batch_xs), y: sess.run(batch_ys)})\n","      # Compute average loss\n","      avg_cost += c / total_batch\n","    # Display logs per epoch step\n","    if epoch % display_step == 0:\n","      print (\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(avg_cost))\n","  print(\"Optimization Finished!\")\n","  # Test model\n","  correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n","  # Calculate accuracy\n","  accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n","  # To keep sizes compatible with model\n","  print (\"Accuracy:\", accuracy.eval({x: sess.run(x_test), y: sess.run(y_test)}))\n","  print(\"Test sample:\")\n","  #Get 28x28 image\n","  print('x_test[47]: ', x_test[47])\n","  sample_1 = tf.reshape(x_test[47], [28,28])\n","  # Get corresponding integer label from one-hot encoded data\n","  sample_label_1 = np.where(sess.run(y_test[47]) == 1)[0][0]\n","  # Plot sample\n","  plt.imshow(sess.run(sample_1), cmap='Greys')\n","  plt.title('label = {}'.format(sample_label_1))\n","  plt.show()\n","  x_test = tf.reshape(x_test[47], [1,784])\n","  predicted_value = tf.argmax(pred, 1)\n","  print(\"is classified as: {}\".format(predicted_value.eval(session=sess, feed_dict={x: sess.run(x_test)})))"],"metadata":{"id":"TAH0dVkBWaTx"},"execution_count":null,"outputs":[]}]}
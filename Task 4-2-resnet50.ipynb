{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1Bel3LVNDVt0MXCun9YqWaRwe-6sFfJ4e","authorship_tag":"ABX9TyO8RoogANhGqGSP/aLKUOeY"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow.keras.applications import resnet50, xception, inception_v3  \n","from keras import Input, Model, layers"],"metadata":{"id":"w2M2ETfTJyoX"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zzn8sH25I6lH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1677609263027,"user_tz":-120,"elapsed":1911,"user":{"displayName":"Trâm Mai","userId":"14047835568893253890"}},"outputId":"07297ad6-786c-407a-ebee-38655a3d3241"},"outputs":[{"output_type":"stream","name":"stdout","text":["Found 3846 files belonging to 12 classes.\n","Using 3077 files for training.\n","Using 769 files for validation.\n"]}],"source":["import pathlib\n","\n","BATCH_SIZE = 32\n","IMG_SIZE = (224, 224)\n","\n","data_dir = pathlib.Path('drive/My Drive/TIES4911/wonders_world/Wonders of World/Wonders of World')\n","(train_ds, val_ds) = tf.keras.utils.image_dataset_from_directory(\n","    data_dir,\n","    shuffle=True,\n","    validation_split=0.2,\n","    subset='both',\n","    image_size=IMG_SIZE,\n","    batch_size=BATCH_SIZE,\n","    seed=123,\n","    label_mode='categorical'\n",")\n","\n","class_names = train_ds.class_names\n","num_classes = len(class_names)"]},{"cell_type":"code","source":["AUTOTUNE = tf.data.AUTOTUNE\n","\n","train_ds = train_ds.prefetch(buffer_size=AUTOTUNE)\n","val_ds = val_ds.prefetch(buffer_size=AUTOTUNE)"],"metadata":{"id":"lfC4m_rBKbsm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for x_batch, y_batch in train_ds.take(1):\n","  print(x_batch.shape)\n","  break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t6ywfnx6NtI_","executionInfo":{"status":"ok","timestamp":1677574344129,"user_tz":-120,"elapsed":2785,"user":{"displayName":"Trâm Mai","userId":"14047835568893253890"}},"outputId":"539d82d9-7727-494d-c53d-16e79affe747"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(32, 180, 180, 3)\n"]}]},{"cell_type":"code","source":["data_augmentation = tf.keras.Sequential([\n","  tf.keras.layers.RandomFlip('horizontal'),\n","  tf.keras.layers.RandomRotation(0.2),\n","])"],"metadata":{"id":"Atz36tmrLVjY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["inputs = Input(shape=(224, 224, 3))\n","\n","base_model = resnet50.ResNet50(include_top=False)\n","base_model.trainable = False\n","\n","preprocess_input = resnet50.preprocess_input\n","global_average_layer = layers.GlobalAveragePooling2D()\n","prediction_layer = layers.Dense(12)\n","\n","x = data_augmentation(inputs)\n","x = preprocess_input(x)\n","x = base_model(x, training=False)\n","x = global_average_layer(x)\n","x = layers.Dropout(0.5)(x)\n","outputs = prediction_layer(x)\n","\n","model = Model(inputs, outputs)\n","model.compile(optimizer='adam',\n","              loss='categorical_crossentropy',\n","              metrics=['accuracy'])\n","\n","checkpoint = tf.train.Checkpoint(model=model)\n","model.summary()\n","\n","loss0, accuracy0 = model.evaluate(val_ds)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YZvCZh2nNWBn","executionInfo":{"status":"ok","timestamp":1677609596071,"user_tz":-120,"elapsed":160442,"user":{"displayName":"Trâm Mai","userId":"14047835568893253890"}},"outputId":"bd2039ff-5dd2-43dc-ba8f-f9cf6e89ed48"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n"]},{"output_type":"stream","name":"stdout","text":["Model: \"model_8\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_18 (InputLayer)       [(None, 224, 224, 3)]     0         \n","                                                                 \n"," sequential_1 (Sequential)   (None, 224, 224, 3)       0         \n","                                                                 \n"," tf.__operators__.getitem_8   (None, 224, 224, 3)      0         \n"," (SlicingOpLambda)                                               \n","                                                                 \n"," tf.nn.bias_add_8 (TFOpLambd  (None, 224, 224, 3)      0         \n"," a)                                                              \n","                                                                 \n"," resnet50 (Functional)       (None, None, None, 2048)  23587712  \n","                                                                 \n"," global_average_pooling2d_1   (None, 2048)             0         \n"," (GlobalAveragePooling2D)                                        \n","                                                                 \n"," dropout_1 (Dropout)         (None, 2048)              0         \n","                                                                 \n"," dense_8 (Dense)             (None, 12)                24588     \n","                                                                 \n","=================================================================\n","Total params: 23,612,300\n","Trainable params: 24,588\n","Non-trainable params: 23,587,712\n","_________________________________________________________________\n","25/25 [==============================] - 156s 6s/step - loss: 7.0061 - accuracy: 0.0572\n"]}]},{"cell_type":"code","source":["print(len(model.trainable_variables))"],"metadata":{"id":"yKPJn2HxWW1L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["checkpoint_path = 'drive/My Drive/TIES4911/task4/task4-1/resnet50-{epoch:04d}'\n","\n","model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n","    filepath=checkpoint_path,\n","    save_weights_only=True,\n","    save_freq=2*97,\n","    verbose=1)\n","\n","history = model.fit(train_ds,\n","                    epochs=10,\n","                    validation_data=val_ds,\n","                    callbacks=[model_checkpoint_callback])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D41y3rEWXCqI","executionInfo":{"status":"ok","timestamp":1677617977262,"user_tz":-120,"elapsed":4737138,"user":{"displayName":"Trâm Mai","userId":"14047835568893253890"}},"outputId":"a9b04647-7c28-457d-dd83-14fdc9eab8d0"},"execution_count":null,"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch 1/10\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n"]},{"output_type":"stream","name":"stdout","text":["97/97 [==============================] - 854s 9s/step - loss: 8.6333 - accuracy: 0.1118 - val_loss: 10.6517 - val_accuracy: 0.1925\n","Epoch 2/10\n","96/97 [============================>.] - ETA: 6s - loss: 10.0396 - accuracy: 0.1396 \n","Epoch 2: saving model to drive/My Drive/TIES4911/task4/task4-1/resnet50-0002\n","97/97 [==============================] - 784s 8s/step - loss: 10.0391 - accuracy: 0.1394 - val_loss: 9.6221 - val_accuracy: 0.1482\n","Epoch 3/10\n","97/97 [==============================] - 825s 9s/step - loss: 10.4756 - accuracy: 0.1241 - val_loss: 11.7640 - val_accuracy: 0.1717\n","Epoch 4/10\n","96/97 [============================>.] - ETA: 6s - loss: 10.2530 - accuracy: 0.1315 \n","Epoch 4: saving model to drive/My Drive/TIES4911/task4/task4-1/resnet50-0004\n","97/97 [==============================] - 816s 8s/step - loss: 10.2626 - accuracy: 0.1313 - val_loss: 9.5494 - val_accuracy: 0.1717\n","Epoch 5/10\n","97/97 [==============================] - 813s 8s/step - loss: 10.5570 - accuracy: 0.1440 - val_loss: 11.6762 - val_accuracy: 0.1938\n","Epoch 6/10\n","96/97 [============================>.] - ETA: 6s - loss: 10.5919 - accuracy: 0.1501 \n","Epoch 6: saving model to drive/My Drive/TIES4911/task4/task4-1/resnet50-0006\n","97/97 [==============================] - 816s 8s/step - loss: 10.5904 - accuracy: 0.1498 - val_loss: 12.3520 - val_accuracy: 0.1990\n","Epoch 7/10\n","97/97 [==============================] - 812s 8s/step - loss: 11.0559 - accuracy: 0.1557 - val_loss: 11.0133 - val_accuracy: 0.1938\n","Epoch 8/10\n","96/97 [============================>.] - ETA: 6s - loss: 10.8932 - accuracy: 0.1507 \n","Epoch 8: saving model to drive/My Drive/TIES4911/task4/task4-1/resnet50-0008\n","97/97 [==============================] - 812s 8s/step - loss: 10.8863 - accuracy: 0.1514 - val_loss: 12.8520 - val_accuracy: 0.2055\n","Epoch 9/10\n","97/97 [==============================] - 819s 8s/step - loss: 10.4474 - accuracy: 0.1521 - val_loss: 12.3034 - val_accuracy: 0.1808\n","Epoch 10/10\n","96/97 [============================>.] - ETA: 6s - loss: 10.5861 - accuracy: 0.1478 \n","Epoch 10: saving model to drive/My Drive/TIES4911/task4/task4-1/resnet50-0010\n","97/97 [==============================] - 816s 8s/step - loss: 10.5847 - accuracy: 0.1475 - val_loss: 9.8205 - val_accuracy: 0.1547\n"]}]},{"cell_type":"code","source":["loss1, accuracy1 = model.evaluate(val_ds)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gVhl8qEstiSB","executionInfo":{"status":"ok","timestamp":1677618147228,"user_tz":-120,"elapsed":166276,"user":{"displayName":"Trâm Mai","userId":"14047835568893253890"}},"outputId":"4ad36b56-5436-46b2-a11e-3172175af0a2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["25/25 [==============================] - 165s 6s/step - loss: 9.8205 - accuracy: 0.1547\n"]}]}]}
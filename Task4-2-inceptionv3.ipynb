{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"0W2tWXEkW5uG","executionInfo":{"status":"ok","timestamp":1677823109147,"user_tz":-120,"elapsed":5222,"user":{"displayName":"Trâm Mai","userId":"14047835568893253890"}}},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow.keras.applications import resnet50, xception, inception_v3  \n","from keras import Input, Model, layers"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5761,"status":"ok","timestamp":1677823118133,"user":{"displayName":"Trâm Mai","userId":"14047835568893253890"},"user_tz":-120},"id":"uedywjTyXCgU","outputId":"95b2c5b7-7016-443e-a749-faaf65938909"},"outputs":[{"output_type":"stream","name":"stdout","text":["Found 3846 files belonging to 12 classes.\n","Using 3077 files for training.\n","Using 769 files for validation.\n"]}],"source":["import pathlib\n","\n","BATCH_SIZE = 32\n","IMG_SIZE = (224, 224)\n","\n","data_dir = pathlib.Path('drive/My Drive/TIES4911/wonders_world/Wonders of World/Wonders of World')\n","(train_ds, val_ds) = tf.keras.utils.image_dataset_from_directory(\n","    data_dir,\n","    shuffle=True,\n","    validation_split=0.2,\n","    subset='both',\n","    image_size=IMG_SIZE,\n","    batch_size=BATCH_SIZE,\n","    seed=123,\n","    label_mode='categorical'\n",")\n","\n","class_names = train_ds.class_names\n","num_classes = len(class_names)"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"5FCh5DsoXHIj","executionInfo":{"status":"ok","timestamp":1677823119807,"user_tz":-120,"elapsed":257,"user":{"displayName":"Trâm Mai","userId":"14047835568893253890"}}},"outputs":[],"source":["AUTOTUNE = tf.data.AUTOTUNE\n","\n","train_ds = train_ds.prefetch(buffer_size=AUTOTUNE)\n","val_ds = val_ds.prefetch(buffer_size=AUTOTUNE)"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"x2eHbbvXXInt","executionInfo":{"status":"ok","timestamp":1677823123289,"user_tz":-120,"elapsed":247,"user":{"displayName":"Trâm Mai","userId":"14047835568893253890"}}},"outputs":[],"source":["data_augmentation = tf.keras.Sequential([\n","  tf.keras.layers.RandomFlip('horizontal'),\n","  tf.keras.layers.RandomRotation(0.2),\n","])"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":91449,"status":"ok","timestamp":1677823218533,"user":{"displayName":"Trâm Mai","userId":"14047835568893253890"},"user_tz":-120},"id":"Ves5heqGXKRF","outputId":"66a94f92-20de-4f80-d2fe-041d6ae4abc0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n","87910968/87910968 [==============================] - 1s 0us/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n"]},{"output_type":"stream","name":"stdout","text":["Model: \"model\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n","                                                                 \n"," sequential (Sequential)     (None, 224, 224, 3)       0         \n","                                                                 \n"," tf.math.truediv (TFOpLambda  (None, 224, 224, 3)      0         \n"," )                                                               \n","                                                                 \n"," tf.math.subtract (TFOpLambd  (None, 224, 224, 3)      0         \n"," a)                                                              \n","                                                                 \n"," inception_v3 (Functional)   (None, 5, 5, 2048)        21802784  \n","                                                                 \n"," global_average_pooling2d (G  (None, 2048)             0         \n"," lobalAveragePooling2D)                                          \n","                                                                 \n"," dropout (Dropout)           (None, 2048)              0         \n","                                                                 \n"," dense (Dense)               (None, 12)                24588     \n","                                                                 \n","=================================================================\n","Total params: 21,827,372\n","Trainable params: 24,588\n","Non-trainable params: 21,802,784\n","_________________________________________________________________\n","25/25 [==============================] - 86s 3s/step - loss: 6.8691 - accuracy: 0.0884\n"]}],"source":["inputs = Input(shape=(224, 224, 3))\n","\n","base_model = inception_v3.InceptionV3(input_tensor=inputs, include_top=False)\n","base_model.trainable = False\n","\n","preprocess_input = inception_v3.preprocess_input\n","global_average_layer = layers.GlobalAveragePooling2D()\n","prediction_layer = layers.Dense(num_classes)\n","\n","x = data_augmentation(inputs)\n","x = preprocess_input(x)\n","x = base_model(x, training=False)\n","x = global_average_layer(x)\n","x = layers.Dropout(0.5)(x)\n","outputs = prediction_layer(x)\n","\n","model = Model(inputs, outputs)\n","model.compile(optimizer='adam',\n","              loss='categorical_crossentropy',\n","              metrics=['accuracy'])\n","\n","checkpoint = tf.train.Checkpoint(model=model)\n","model.summary()\n","\n","loss0, accuracy0 = model.evaluate(val_ds)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":201,"status":"ok","timestamp":1677611091932,"user":{"displayName":"Trâm Mai","userId":"14047835568893253890"},"user_tz":-120},"id":"_oX0jMxEXMG5","outputId":"e6c99550-30be-47a6-818b-c882ee2a7b0b"},"outputs":[{"name":"stdout","output_type":"stream","text":["2\n"]}],"source":["print(len(model.trainable_variables))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mHvJf5QgXNvZ","executionInfo":{"status":"ok","timestamp":1677614858028,"user_tz":-120,"elapsed":1617712,"user":{"displayName":"Trâm Mai","userId":"14047835568893253890"}},"outputId":"5661c806-121b-4c91-c8f6-78a6598f5a99"},"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch 1/10\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n","WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n"]},{"output_type":"stream","name":"stdout","text":["97/97 [==============================] - 374s 4s/step - loss: 8.9200 - accuracy: 0.0715 - val_loss: 10.8823 - val_accuracy: 0.0923\n","Epoch 2/10\n","96/97 [============================>.] - ETA: 2s - loss: 9.6165 - accuracy: 0.0960\n","Epoch 2: saving model to drive/My Drive/TIES4911/task4/task4-2/inceptionv3-0002\n","97/97 [==============================] - 359s 4s/step - loss: 9.6218 - accuracy: 0.0962 - val_loss: 11.5909 - val_accuracy: 0.1443\n","Epoch 3/10\n","97/97 [==============================] - 344s 4s/step - loss: 9.6941 - accuracy: 0.1141 - val_loss: 10.3751 - val_accuracy: 0.1456\n","Epoch 4/10\n","96/97 [============================>.] - ETA: 2s - loss: 9.9935 - accuracy: 0.1195\n","Epoch 4: saving model to drive/My Drive/TIES4911/task4/task4-2/inceptionv3-0004\n","97/97 [==============================] - 358s 4s/step - loss: 10.0034 - accuracy: 0.1193 - val_loss: 10.5847 - val_accuracy: 0.1625\n","Epoch 5/10\n","97/97 [==============================] - 357s 4s/step - loss: 10.3007 - accuracy: 0.1212 - val_loss: 11.7794 - val_accuracy: 0.1599\n","Epoch 6/10\n","96/97 [============================>.] - ETA: 2s - loss: 10.1854 - accuracy: 0.1178\n","Epoch 6: saving model to drive/My Drive/TIES4911/task4/task4-2/inceptionv3-0006\n","97/97 [==============================] - 357s 4s/step - loss: 10.1898 - accuracy: 0.1183 - val_loss: 12.1776 - val_accuracy: 0.1612\n","Epoch 7/10\n","97/97 [==============================] - 359s 4s/step - loss: 10.3466 - accuracy: 0.1306 - val_loss: 12.1567 - val_accuracy: 0.1573\n","Epoch 8/10\n","96/97 [============================>.] - ETA: 2s - loss: 10.0329 - accuracy: 0.1243\n","Epoch 8: saving model to drive/My Drive/TIES4911/task4/task4-2/inceptionv3-0008\n","97/97 [==============================] - 359s 4s/step - loss: 10.0375 - accuracy: 0.1241 - val_loss: 12.0309 - val_accuracy: 0.1560\n","Epoch 9/10\n","97/97 [==============================] - 358s 4s/step - loss: 10.5872 - accuracy: 0.1258 - val_loss: 11.3393 - val_accuracy: 0.1495\n","Epoch 10/10\n","96/97 [============================>.] - ETA: 2s - loss: 10.3951 - accuracy: 0.1234\n","Epoch 10: saving model to drive/My Drive/TIES4911/task4/task4-2/inceptionv3-0010\n","97/97 [==============================] - 345s 4s/step - loss: 10.4044 - accuracy: 0.1232 - val_loss: 12.7016 - val_accuracy: 0.1443\n"]}],"source":["checkpoint_path = 'drive/My Drive/TIES4911/task4/task4-2/inceptionv3-{epoch:04d}'\n","\n","model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n","    filepath=checkpoint_path,\n","    save_weights_only=True,\n","    save_freq=2*97,\n","    verbose=1)\n","\n","history = model.fit(train_ds,\n","                    epochs=10,\n","                    validation_data=val_ds,\n","                    callbacks=[model_checkpoint_callback])"]},{"cell_type":"code","source":["loss1, accuracy1 = model.evaluate(val_ds)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SLbBaMPRtoLi","executionInfo":{"status":"ok","timestamp":1677616212040,"user_tz":-120,"elapsed":82554,"user":{"displayName":"Trâm Mai","userId":"14047835568893253890"}},"outputId":"9399f09a-f601-4a76-aad7-3ebb2edf48b4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["25/25 [==============================] - 68s 3s/step - loss: 12.7016 - accuracy: 0.1443\n"]}]}],"metadata":{"colab":{"provenance":[],"mount_file_id":"1yf7rxy-LQoAoijqJ1x4_236lwfK4o0q-","authorship_tag":"ABX9TyOhb+5Qr+cxCllbzzxi07Lv"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}